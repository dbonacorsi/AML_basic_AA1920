{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML_8_Modelling_CompareModels.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7An3uCq3BbQF","colab_type":"text"},"source":["# Compare ML Algorithms"]},{"cell_type":"markdown","metadata":{"id":"RMT63yygBbQG","colab_type":"text"},"source":["It is important to compare the respective performance of multiple different ML algorithms consistently. \n","\n","We will discover how you can create a test harness to compare multiple different ML algorithms in Python with sklearn. You can use this test harness as a template on your own ML problems and add more and different algorithms to compare. \n","\n","So, the goal is to learn:\n","1. How to formulate an experiment to directly compare ML algorithms\n","2. How to build a reusable template for evaluating the performance of multiple algorithms on one dataset\n","3. How to report and visualize the results when comparing algorithm performance."]},{"cell_type":"markdown","metadata":{"id":"LbZQ4AZxBbQH","colab_type":"text"},"source":["## Choose \"the best\" ML algorithm"]},{"cell_type":"markdown","metadata":{"id":"PvIHI_W7Q1hA","colab_type":"text"},"source":["You should use a number of different ways of looking at the estimated accuracy of your ML algorithms in order to choose the one or two algorithm to finalize. A way to do this is to use visualization methods to show the average accuracy, variance and other properties of the distribution of model accuracies. \n","\n","We will discover how you can do that in Python with scikit-learn."]},{"cell_type":"markdown","metadata":{"id":"V9uyvtKcBbQI","colab_type":"text"},"source":["## Consistent comparison of ML algos"]},{"cell_type":"markdown","metadata":{"id":"JM58JfmaRD-y","colab_type":"text"},"source":["In the example below 6 different classification algorithms are compared on a single dataset:\n","\n","* Logistic Regression\n","* Linear Discriminant Analysis\n","* k-Nearest Neighbors\n","* Classification and Regression Trees\n","* Naive Bayes\n","* Support Vector Machines\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O0yWhosvRjwN","colab_type":"text"},"source":["The dataset is the diabetes one. The problem has 2 classes and 8 numeric input variables of varying scales. The 10-fold cross-validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithm is evaluated in precisely the same way. Each algorithm is given a short name, useful for summarizing results afterward."]},{"cell_type":"markdown","metadata":{"id":"73BUWzHZRcPh","colab_type":"text"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"VOTtvlZKReno","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML_basic_AA1920/master/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhQZaywWR0hw","colab_type":"code","colab":{}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4aVDUBrBbQJ","colab_type":"code","colab":{}},"source":["#from pandas import read_csv\n","from matplotlib import pyplot\n","#\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","#\n","from sklearn.linear_model import LogisticRegression                         # <---\n","#\n","from sklearn.tree import DecisionTreeClassifier                             # <---\n","#\n","from sklearn.neighbors import KNeighborsClassifier                          # <---\n","#\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis        # <---\n","#\n","from sklearn.naive_bayes import GaussianNB                                  # <---\n","#\n","from sklearn.svm import SVC                                                 # <---"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzX0x44jTArw","colab_type":"text"},"source":["Everything meaningful is in this cell:"]},{"cell_type":"code","metadata":{"id":"Na-zQ2SxBbQN","colab_type":"code","colab":{}},"source":["# Compare Algorithms\n","\n","# prepare models\n","models = []\n","models.append(( 'LR'   , LogisticRegression(solver='lbfgs', max_iter=500)))    # avoid warnings with (solver='lbfgs', max_iter=500)\n","models.append(( 'LDA'  , LinearDiscriminantAnalysis()))\n","models.append(( 'KNN'  , KNeighborsClassifier()))\n","models.append(( 'CART' , DecisionTreeClassifier()))\n","models.append(( 'NB'   , GaussianNB()))\n","models.append(( 'SVM' , SVC()))                                                # avoid warnings with (gamma='scale')\n","\n","# evaluate each model in turn\n","results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","  kfold = KFold(n_splits=10, random_state=7)\n","  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","  results.append(cv_results)\n","  names.append(name)\n","  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","  print(msg)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbS1-c1gBbQR","colab_type":"text"},"source":["Running the example provides a list of each algorithm short name, the mean accuracy and the standard deviation accuracy. "]},{"cell_type":"code","metadata":{"id":"rKWO99ltTWF4","colab_type":"code","colab":{}},"source":["# boxplot algorithm comparison\n","fig = pyplot.figure()\n","fig.suptitle('Algorithms comparison')\n","ax = fig.add_subplot(111)\n","pyplot.boxplot(results)\n","ax.set_xticklabels(names)\n","pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s39Uzuu4Td7q","colab_type":"text"},"source":["The example also provides a box and whisker plot showing the spread of the accuracy scores across each cross-validation fold for each algorithm. From these results, a suggestion could easily arise: **which models are worthy of further study on this problem?**"]},{"cell_type":"markdown","metadata":{"id":"PmZWKQBaBbQR","colab_type":"text"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"idyMH0jfBbQS","colab_type":"text"},"source":["What we did:\n","\n","* we discovered how to evaluate multiple different ML algorithms on a dataset in Python with scikit-learn. You learned how to both use the same test harness to evaluate the algorithms and how to summarize the results both numerically and using a box and whisker plot. You can use this recipe as a template for evaluating multiple algorithms on your own problems."]}]}