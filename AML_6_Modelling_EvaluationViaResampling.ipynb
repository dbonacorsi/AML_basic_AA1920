{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML_6_Modelling_EvaluationViaResampling.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DAcD2u6uwVxh","colab_type":"text"},"source":["# Evaluate the performance of ML algos with Resampling"]},{"cell_type":"markdown","metadata":{"id":"6NyHWASMpbyq","colab_type":"text"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"egGmPYr6pblR","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML_basic_AA1920/master/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_hXMsaB0wVxk","colab_type":"text"},"source":["## 1. Split into Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"2Qu_ml5owVxl","colab_type":"code","colab":{}},"source":["#from pandas import read_csv\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yzT3ldEtwVxo","colab_type":"code","colab":{}},"source":["# data import\n","#filename = 'pima-indians-diabetes.data.csv'\n","#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","#dataframe = read_csv(filename, names=names)\n","#array = dataframe.values\n","array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVBhj9VXwVxr","colab_type":"code","colab":{}},"source":["# prepare for the evaluation with a train and test set\n","test_size = 0.33\n","seed = 7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQ2GUpzjwVxu","colab_type":"code","colab":{}},"source":["# Evaluate using a train and a test set\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n","model = LogisticRegression()            # choose a model\n","model.fit(X_train, Y_train)             # train on the training set\n","result = model.score(X_test, Y_test)    # get accuracy as measured on the test set\n","print(\"Accuracy: %.3f%%\" % (result*100.0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMPTbXT1wVxx","colab_type":"text"},"source":["## <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"lBLCtCkawVxx","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\">\n","Try to change the seed, and re-train. Does accuracy change? Is it reproducible for a a fixed seed? for different seeds, could you measure its variance? (up to your curiosity here, but no need to do more here than just few tries and get a feeling.. but you can do more and clever tests..)\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"pRMUpARzwVxx","colab_type":"text"},"source":["## <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"m2qqRJk-xhJH","colab_type":"code","colab":{}},"source":["# type your code below"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k59kKKq_wVyA","colab_type":"text"},"source":["## <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"kflKRgO0wVyA","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\">\n","What happens if I check accuracy on the _train_ set (conceptually wrong)? Do I see something different or not? What is the drawback if I do this mistake?\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"9KtosMMGwVyB","colab_type":"text"},"source":["## <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"nUH8RXqB09R_","colab_type":"code","colab":{}},"source":["# type your code below"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zNCxbHA1wVyP","colab_type":"text"},"source":["## <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"EaSn-xI3wVyP","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\">\n","What if change the training/test ratio?\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"KjWTA2viwVyQ","colab_type":"text"},"source":["## <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"6mnkaabe0-kQ","colab_type":"code","colab":{}},"source":["# type your code below"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0-YQZkAwVyW","colab_type":"text"},"source":["## 2. K-fold Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"6L7CDFjkzQYn","colab_type":"text"},"source":["It works by **splitting the dataset into k-parts** (e.g. $k=5$ or $k=10$). Each split of the data is called a $fold$. The algorithm is trained on $k-1$ folds (with 1 held back), and then tested on the held-back fold. This is also repeated, so that _each_ fold of the dataset is given a chance to be the held-back test set. So you repeat it k times. After running cross-validation you end up with $k$ different performance scores that you can summarize using a mean and a standard deviation. \n"]},{"cell_type":"code","metadata":{"id":"Y9tS9Sb0wVyW","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score   # <---\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLE_nHF5wVyZ","colab_type":"code","colab":{}},"source":["# Evaluate using Cross Validation\n","num_folds = 70\n","seed = 7\n","\n","kfold = KFold(n_splits=num_folds, random_state=seed)\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_tYK3AWwVyb","colab_type":"text"},"source":["You can see that we report both the mean and the standard deviation of the performance measure.\n"]},{"cell_type":"markdown","metadata":{"id":"WXGOB_6qwVyb","colab_type":"text"},"source":["## <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"RbpMa5EKwVyc","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\">\n","What if I change the nb folds?\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Fn5Lnjx0wVyc","colab_type":"text"},"source":["## <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"2O1NeZfW0_ui","colab_type":"code","colab":{}},"source":["# type your code below"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jDFgpsWwVye","colab_type":"text"},"source":["## 3. Leave One Out Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"TsI_mVLWwVyf","colab_type":"text"},"source":["You can configure cross-validation so that the size of the fold is 1 ($k=n$, i.e. $k$ is set to the number of observations in your dataset). "]},{"cell_type":"code","metadata":{"id":"80lH8gNLwVyf","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import LeaveOneOut       # <---\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B92zPwKwwVyh","colab_type":"code","colab":{}},"source":["# Evaluate using Leave One Out Cross Validation\n","loocv = LeaveOneOut()\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=loocv)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4gRjJOvwVyj","colab_type":"text"},"source":["(*NOTE: probably not so visible in this small example, but the time it took to run this is larger than the previous one..*)\n","\n","You can see in the standard deviation that the score has **higher variance** than the k-fold cross-validation results described above."]},{"cell_type":"markdown","metadata":{"id":"u6gwM2Z4wVyk","colab_type":"text"},"source":["## 4. Repeated Random Test-Train Splits"]},{"cell_type":"markdown","metadata":{"id":"ku79ofdQz-0B","colab_type":"text"},"source":["Another variation on k-fold cross-validation is to **create a random split of the data** like the train/test split described above, but **repeat multiple times the process of splitting and evaluation of the algorithm** , like cross-validation."]},{"cell_type":"code","metadata":{"id":"ZKBoxgB3wVyk","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import ShuffleSplit      # <---\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKVeSQ3SwVym","colab_type":"code","colab":{}},"source":["# Evaluate using Shuffle Split Cross Validation\n","n_splits = 100\n","test_size = 0.33\n","seed = 7\n","\n","kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tni5i-f_wVyo","colab_type":"text"},"source":["We can see that in this case the distribution of the performance measure is on par with\n","k-fold cross-validation above."]},{"cell_type":"markdown","metadata":{"id":"gmbTN4qiwVyo","colab_type":"text"},"source":["## OK, fine, but.. what techniques to use when?!?"]},{"cell_type":"markdown","metadata":{"id":"s0ePQcYT0IZX","colab_type":"text"},"source":["Discussion at the lecture."]},{"cell_type":"markdown","metadata":{"id":"L86TrKElwVyp","colab_type":"text"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"9Wmx7bsxwVyp","colab_type":"text"},"source":["What we did:\n","\n","* we discovered 4 statistical techniques that we can use to estimate the performance of ML algorithms, called Resampling. "]},{"cell_type":"markdown","metadata":{"id":"AvXL00WCwVyp","colab_type":"text"},"source":["## What's next "]},{"cell_type":"markdown","metadata":{"id":"KaIOetr1wVyq","colab_type":"text"},"source":["Now we will see how you can evaluate the performance of classification and regression algorithms using a suite of different metrics and built in evaluation reports."]}]}