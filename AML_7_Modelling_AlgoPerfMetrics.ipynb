{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML_7_Modelling_AlgoPerfMetrics.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"G-txJJP7H_hJ","colab_type":"text"},"source":["# ML Algorithm Performance Metrics"]},{"cell_type":"markdown","metadata":{"id":"oarLVuEcH_hL","colab_type":"text"},"source":["## Algorithm Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"2M18Wz-HIk5G","colab_type":"text"},"source":["We need more input datasets:\n","* For CLASSIFICATION metrics: we use the Pima Indians onset of diabetes dataset\n","* For REGRESSION metrics: we use the Boston House Price dataset \n"]},{"cell_type":"markdown","metadata":{"id":"we0oQqeDJSSZ","colab_type":"text"},"source":["We do not focus on modelling utself, in this notebook, so we use ***Logistic Regression*** for the classification problem and ***Linear Regression*** for the regression problems. \n","\n","A 10-fold CV test harness is used to demonstrate each metric (because this is a  likely scenario you will use when employing different algorithm evaluation metrics)\n","\n","More about ML algorithm performance metrics supported by scikit-learn can be found [here](http://scikit-learn.org/stable/modules/model_evaluation.html) on the page \"Model evaluation: quantifying the quality of predictions\". "]},{"cell_type":"markdown","metadata":{"id":"S6L3iCx6H_hM","colab_type":"text"},"source":["# CLASSIFICATION Metrics"]},{"cell_type":"markdown","metadata":{"id":"Fy2LelSIPnS1","colab_type":"text"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"D_0zvG3kPnJl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML_basic_AA1920/master/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faE6yJgKH_hN","colab_type":"text"},"source":["We will review how to use the following metrics:\n","* [CLAS-1] Classification Accuracy\n","* [CLAS-2] Logarithmic Loss\n","* [CLAS-3] Area Under ROC Curve\n","* [CLAS-4] Confusion Matrix\n","* [CLAS-5] Classification Report"]},{"cell_type":"markdown","metadata":{"id":"MrMybQseH_hO","colab_type":"text"},"source":["## [CLAS-1] Classification Accuracy"]},{"cell_type":"markdown","metadata":{"id":"24UleZkhH_hO","colab_type":"text"},"source":["Classification accuracy is **the number of correct predictions made as a ratio of all predictions made**. \n","\n","This is the most common evaluation metric for classification problems, and it is also often the most misused."]},{"cell_type":"markdown","metadata":{"id":"8IHgnXlpPQY1","colab_type":"text"},"source":["Below is an example of calculating classification accuracy."]},{"cell_type":"code","metadata":{"id":"QIoQ8VxaH_hP","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","#\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsvmTIQNP8Oz","colab_type":"code","colab":{}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwvS5XSLaxOc","colab_type":"code","colab":{}},"source":["seed = 7"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqctOAhTcTiJ","colab_type":"text"},"source":["We do k-fold CV."]},{"cell_type":"code","metadata":{"id":"8gx6xhKflHat","colab_type":"code","colab":{}},"source":["kfold = KFold(n_splits=10, random_state=seed)\n","model = LogisticRegression()\n","#model = LogisticRegression(solver='lbfgs', max_iter=500)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"trt_zhuJH_hT","colab_type":"code","colab":{}},"source":["# Cross Validation Classification Accuracy\n","scoring = 'accuracy'                                             # <--- \n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7UZCPMcH_hW","colab_type":"text"},"source":["You can see that the accuracy ratio is reported: we built a model that is approximately 78% accurate."]},{"cell_type":"markdown","metadata":{"id":"UrZFXTsdckgO","colab_type":"text"},"source":["## <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"TX5jWvreckGI","colab_type":"text"},"source":["Measure the time it takes to run the previous cell, by running k-fold CV with different k's, and compare timing and accuracies obtained."]},{"cell_type":"markdown","metadata":{"id":"NlR6mU7wcne2","colab_type":"text"},"source":["## <font color='green'>Solution 1</font>"]},{"cell_type":"code","metadata":{"id":"k7KKhaGCcmrq","colab_type":"code","colab":{}},"source":["# enter your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKNM45TQH_hX","colab_type":"text"},"source":["## [CLAS-2] Logarithmic Loss (aka \"logloss\")"]},{"cell_type":"markdown","metadata":{"id":"_jOB8f1-H_hX","colab_type":"text"},"source":["Logarithmic loss (or logloss) is **a performance metric for evaluating the predictions of probabilities of membership to a given class**."]},{"cell_type":"markdown","metadata":{"id":"jlQw2gNbjO32","colab_type":"text"},"source":["Below is an example of calculating logloss for Logistic regression predictions on the Pima Indians onset of diabetes dataset.\n"]},{"cell_type":"code","metadata":{"id":"i2eytcFcH_hY","colab_type":"code","colab":{}},"source":["# Cross Validation Classification LogLoss\n","scoring = 'neg_log_loss'                      #<---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feW0tkQDH_hb","colab_type":"text"},"source":["Smaller logloss is better with 0 representing a perfect logloss. The\n","measure is inverted to be ascending when using the `cross_val_score()` function (see the documentation)."]},{"cell_type":"markdown","metadata":{"id":"dXrSnEVEH_hb","colab_type":"text"},"source":["## [CLAS-3] Area Under ROC Curve"]},{"cell_type":"markdown","metadata":{"id":"5W0vSUv1H_hc","colab_type":"text"},"source":["Area under ROC Curve (or AUC for short) is **a performance metric for binary classification problems**. \n","\n","ROC can be broken down into **sensitivity** and **specificity**. A binary classification problem is really a trade-off between sensitivity and specificity.\n","* Sensitivity is the true positive rate (TPR) also called the Recall. It is the number of instances from the positive (first) class that actually predicted correctly.\n","* Specificity is also called the true negative rate (TNR). It is the number of instances from the\n","negative (second) class that were actually predicted correctly. \n","\n","The AUC represents a modelâ€™s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. \n","\n","The example below provides a demonstration of calculating AUC."]},{"cell_type":"code","metadata":{"id":"Ti20zvTkH_hc","colab_type":"code","colab":{}},"source":["# Cross Validation Classification ROC AUC\n","scoring = 'roc_auc'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxPGLPc6H_hf","colab_type":"text"},"source":["You can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skills in the predictions."]},{"cell_type":"markdown","metadata":{"id":"DFGrZupoH_hg","colab_type":"text"},"source":["## [CLAS-3] Confusion Matrix"]},{"cell_type":"markdown","metadata":{"id":"AAU4XTpgH_hg","colab_type":"text"},"source":["The confusion matrix is **a handy (and more informative) presentation of the accuracy of a model with two or more classes**. \n","\n","The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a ML algorithm. See the frontal lectures for some examples.\n","\n","Below is an example of calculating a confusion matrix for a set of predictions by a Logistic Regression on the Pima Indians onset of diabetes dataset."]},{"cell_type":"markdown","metadata":{"id":"PUQWoyvGC6eC","colab_type":"text"},"source":["There are (at least) 2 different ways to do so\n","*   NOTE: to compare these two approaches and avoid to do mistakes, we need to re-execute (or just write again for clarity) some cells above - unnecessary if you do just one method, of course..\n"]},{"cell_type":"markdown","metadata":{"id":"Cq5wI17fIGjX","colab_type":"text"},"source":["### First method"]},{"cell_type":"markdown","metadata":{"id":"FGDPAwL8C-a1","colab_type":"text"},"source":["The first is not to rely on `cross_val_score` at all: there is no option to have a confusion matrix as scoring function in its call after having done the k-fold CV, so one way is not to do CV at all,  opt for a static splitting and validation, then use `confusion_matrix` directly."]},{"cell_type":"code","metadata":{"id":"KzVNaWxbDvaA","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split      # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ogz6JAG4C-FJ","colab_type":"code","colab":{}},"source":["test_size = 0.33\n","\n","# Cross Validation Classification Confusion Matrix\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=500)\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","matrix1 = confusion_matrix(Y_test, predicted)              # <---\n","print(matrix1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-oYYi-T6H_hm","colab_type":"text"},"source":["Although the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions)."]},{"cell_type":"markdown","metadata":{"id":"WXzz84ZbIJWf","colab_type":"text"},"source":["### Second method"]},{"cell_type":"markdown","metadata":{"id":"ipuz93TsEMaz","colab_type":"text"},"source":["The second is keep doing k-fold CV, but to drop the use of `cross_val_score` in favour of `cross_val_predict`.\n"]},{"cell_type":"code","metadata":{"id":"MuWMtsTzH_hh","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold                 # <---\n","from sklearn.model_selection import cross_val_predict     # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xtwfoCzBgRn","colab_type":"code","colab":{}},"source":["kfold = KFold(n_splits=4, random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=300)\n","\n","predicted = cross_val_predict(model, X, Y, cv=kfold)    # <--- NOTE: no 'scoring'\n","matrix2 = confusion_matrix(Y, predicted)\n","print(matrix2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OfnCOyaIlK6","colab_type":"text"},"source":["Same as above: the majority of the predictions fall on the diagonal line of the matrix. Good."]},{"cell_type":"markdown","metadata":{"id":"vbchSWFwHquU","colab_type":"text"},"source":["Let's make a couple of plots. We discuss later."]},{"cell_type":"code","metadata":{"id":"XoqjW7WYHK5-","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix1)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AauTmkozHe_Y","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix2)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBkJ2ZEq61vx","colab_type":"text"},"source":["## <font color='red'>Exercise 2</font>"]},{"cell_type":"markdown","metadata":{"id":"kh6KgPD4Hj0Q","colab_type":"text"},"source":["The 2 matrices are not the same, though, aren't they? So: are the 2 results, content-wise, the same? or comparable?\n"]},{"cell_type":"markdown","metadata":{"id":"4UKxoLEZ637j","colab_type":"text"},"source":["## <font color='green'>Solution 2</font>"]},{"cell_type":"code","metadata":{"id":"FDFM48xA7IkV","colab_type":"code","colab":{}},"source":["# write your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"691f7Co3H_hn","colab_type":"text"},"source":["There is also **a convenience report provided by the scikit-learn library** when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The `classification report()` function displays the precision, recall, F1-score and support for each class. \n","\n","The example below demonstrates the report on the binary classification problem."]},{"cell_type":"code","metadata":{"id":"0NeMkK_3H_ho","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report               # <---"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKep3pyVH_hr","colab_type":"code","colab":{}},"source":["test_size = 0.33\n","seed = 7\n","\n","# Cross Validation Classification Report\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n","    random_state=seed)\n","model = LogisticRegression()\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","report = classification_report(Y_test, predicted)               # <---\n","print(report)                  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CO8RxwPlH_hu","colab_type":"text"},"source":["You can see good prediction and recall for the algorithm."]},{"cell_type":"markdown","metadata":{"id":"d9hZrzyMH_hu","colab_type":"text"},"source":["# REGRESSION Metrics"]},{"cell_type":"markdown","metadata":{"id":"9tUWR8cj979e","colab_type":"text"},"source":["In the regression examples, we will use the Boston house price dataset, which you can find (original source) [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data), and for your convenience it is already in the github repo of the course:\n","\n","* https://raw.githubusercontent.com/dbonacorsi/AML_basic_AA1920/master/datasets/housing.data.csv \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"we42m9SZ8DIe"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F5BbQyy-8DIg","colab":{}},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML_basic_AA1920/master/datasets/housing.data.csv'\n","\n","names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n","data = pd.read_csv(url, delim_whitespace=True, names=names)\n","data\n","\n","#array = dataframe.values\n","#X = array[:,0:13]\n","#Y = array[:,13]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rntb-1qWH_hv","colab_type":"text"},"source":["Here we will review 3 of the most common metrics for evaluating predictions on regression ML problems:\n","* [REGR-1] Mean Absolute Error\n","* [REGR-2] Mean Squared Error\n","* [REGR-3] $R^2$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SEl6bbYoH_hv","colab_type":"text"},"source":["## [REGR-1] Mean Absolute Error"]},{"cell_type":"markdown","metadata":{"id":"t13d5LAvH_hw","colab_type":"text"},"source":["The Mean Absolute Error (or MAE) is **the sum of the absolute differences between predictions and actual values**. "]},{"cell_type":"markdown","metadata":{"id":"fyzznaLS_rgP","colab_type":"text"},"source":["The example below demonstrates calculating mean absolute error on the house dataset."]},{"cell_type":"code","metadata":{"id":"1xoyhUzsH_hw","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOPJHxVnH_h1","colab_type":"code","colab":{}},"source":["# Cross Validation Regression MAE\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_absolute_error'                                  # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MAE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfhhJqhVH_h3","colab_type":"text"},"source":["A value of 0 indicates no error or perfect predictions. Like log-loss, this metric is inverted by\n","the `cross_val_score()` function."]},{"cell_type":"markdown","metadata":{"id":"k-pivqOUH_h4","colab_type":"text"},"source":["## [REGR-2] Mean Squared Error"]},{"cell_type":"markdown","metadata":{"id":"LL-RaegGH_h5","colab_type":"text"},"source":["The Mean Squared Error (or MSE) is much like the MAE in that **it provides a gross idea of the magnitude of error**. "]},{"cell_type":"markdown","metadata":{"id":"u5WMIXHeALJv","colab_type":"text"},"source":["\n","The example below provides a demonstration of calculating MSE."]},{"cell_type":"code","metadata":{"id":"AYdJ01hMH_h5","colab_type":"code","colab":{}},"source":["#num_folds = 10       # a remnant..\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_squared_error'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MSE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SalKxNvNH_h8","colab_type":"text"},"source":["This metric too is inverted so that the results are increasing. "]},{"cell_type":"markdown","metadata":{"id":"nBBBy7LfH_h8","colab_type":"text"},"source":["## [REGR-3] $R^2$ metric"]},{"cell_type":"markdown","metadata":{"id":"6tZ8RVQmH_h9","colab_type":"text"},"source":["The $R^2$ (or R Squared) metric provides **an indication of the goodness of fit of a set of predictions to the actual values**.\n"]},{"cell_type":"markdown","metadata":{"id":"fWsHA29oAlky","colab_type":"text"},"source":["\n","The example below provides a demonstration of calculating the mean $R^2$ for a set of predictions."]},{"cell_type":"code","metadata":{"id":"r21teffhH_h9","colab_type":"code","colab":{}},"source":["# Cross Validation Regression R^2\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'r2'                                                     # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"R^2: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPPk368jH_iA","colab_type":"text"},"source":["You can see the predictions have a poor fit to the actual values with a value closer to zero and less than 0.5."]},{"cell_type":"markdown","metadata":{"id":"_HZhOq_NH_iA","colab_type":"text"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"7qksn4XwH_iB","colab_type":"text"},"source":["What we did:\n","\n","* we discovered metrics that you can use to evaluate your ML algorithms. We learned about 3 classification metrics (Accuracy, LogLoss and AUC) and 2 convenience methods for classification prediction results (Confusion Matrix and Classification Report), as well as 3 metrics for regression problems (MAE, MSE, R2)."]}]}